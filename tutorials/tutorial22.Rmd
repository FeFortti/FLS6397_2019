---
title: 'Tutorial 22'
output: html_document
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE, eval=F, include=T)
```

# Apresentação do problema

Agora que sabemos coletar de uma página da busca atributos e os conteúdos de uma "tag", precisamos repetir o procedimento para todas as páginas de resultado.  Fizemos algo semelhante no Tutorial 1, mas ainda usávamos a função _html\_table_. O objetivo é repetir o que fizemos lá, mas com as novas funções que vimos ao longo do Tutorial 21. Para tanto, vamos usar o "for loop" do Tutorial 20 para ir de uma página a outra.

O primeiro passo é, mais uma vez, ter o nosso link da pesquisa que queremos coletar armazenado em um objeto.  Retiraremos do url o valor do parâmetro "sr" ao final, pois é este que indica quais resultados da página aparecerão.

```{r}
url_base <- 'http://search.folha.uol.com.br/search?q=merenda&site=todos&results_count=3769&search_time=0.033&url=http%3A%2F%2Fsearch.folha.uol.com.br%2Fsearch%3Fq%3Dmerenda%26site%3Dtodos&sr='
```

## Função Paste

Como é possível reparar, o número da página fica ao final do link, por isso podemos utilizar a função chamada _paste0_ ou "colar sem separação" em vez da função _str\_replace_. Note que, diferentemente desta, a função _paste0_ aceita um número ao concatenar seus elementos.

Na linguagem R, escreveremos:

```{r}
i <- 1

url_folha <- paste0(url_base, i)
```

A "url_base" é o endereço da página de busca e o "i" é o contador numérico que utilizaremos em um loop a seguir.

## Coletando o conteúdo e o atributo de todos os links

A lógica de coleta de atributos e conteúdos de um node é a mesma do tutorial anterior. A única diferença é que precisamos aplicar isso para todas as páginas. Agora que temos a url construída, podemos montar um "for loop" que fará isso para nós.

Antes de prosseguir, vamos observar o url da página de busca (poderíamos buscar termos chave, mas, neste caso, vamos pegar todas as notícias relacionadas a eleições). Na página 2 da busca vemos que o final é "sr=26". Na página 3 o final é "sr=51". Há um padrão: as buscas são realizadas de 25 em 25. A a 167a. é última página da busca -- você pode descobrir isso inspecionando manualmente o site. Para "passarmos" de página em página, portanto, temos que ter um "loop" que conte não mais de 1 até 167, mas na seguinte sequência numérica: {1, 26, 51, 76, ..., 4126, 4151}.

Precisamos, então, que "i" seja recalculado dentro do loop para coincidir com a numeração da primeira notícia de cada página. Parece difícil, mas é extremamente simples. Veja o loop abaixo, que imprime a sebaseia desejada multiplicando (i - 1) por 25 e somando 1


```{r}
for (i in 1:167){
  i <- (i - 1) * 25 + 1
  print(i)
}
```

O que precisamos agora é incluir nas "instruções do loop" o que foi discutido no tutorial 2. 

Em primeiro lugar, construímos o url de cada página do resultado da busca:

```{r}
url_folha <- paste0(url_base, i)
```

A seguir, capturamos o código HTML da página:

```{r}
pagina <- read_html(url_folha)
```

Escolhemos apenas os "nodes" que nos interessam:

```{r}
nodes_titulos <- html_nodes(pagina, xpath = '//ol/li/div/div[@class="c-headline__content"]/a/h2')
nodes_links <- html_nodes(pagina, xpath = '//ol/li/div/div[@class="c-headline__content"]/a')
```

Extraímos os títulos e os links com as funções apropriadas:

```{r}
titulos <- html_text(nodes_titulos)
links <- html_attr(nodes_links, name = "href")
```

Combinamos os dois vetores em um data frame:

```{r}
tabela_titulos <- data.frame(titulos, links)
```

Falta "empilhar" o que produziremos em cada iteração do loop de uma forma que facilite a visualização. Criamos um objeto vazio antes do loop. 

Usaremos a função _bind\_rows_ (ou _rbind_ se estiver com problemas com o _dplyr_) para combinar data frames. A cada página agora, teremos 25 resultados em uma tabela com duas variáveis. O que queremos é a junção dos 25 resultados de cada uma das 167 páginas. Vamos também chamar a biblioteca _dplyr_ para usar sua função _bind\_rows_.

```{r}
library(dplyr)
dados_pesquisa <- bind_rows(dados_pesquisa, tabela_titulos)
```

Chegou o momento de colocar dentro loop tudo o que queremos que execute em cada uma das vezes que ele ocorrer. Ou seja, que imprima na tela a página que está executando, que a URL da página de resultados seja construída com a função paste, para todas elas o código HTML seja examinado, lido no R e transformado em objeto XML, colete todos os links e todos os títulos e que "empilhe". Lembrando que não podemos esquecer de definir a URK que estamos usando e criar um data frame vazio para colocar todos os links e títulos coletados antes de iniciar o loop.

```{r}
url_base <- 'http://search.folha.uol.com.br/search?q=merenda&site=todos&results_count=3769&search_time=0.033&url=http%3A%2F%2Fsearch.folha.uol.com.br%2Fsearch%3Fq%3Dmerenda%26site%3Dtodos&sr='

dados_pesquisa <- data_frame()

for (i in 1:167){
  
  print(i)

  i <- (i - 1) * 25 + 1
  
  url_folha <- paste0(url_base, i)
  
  pagina <- read_html(url_folha)
  
  nodes_titulos <- html_nodes(pagina, xpath = '//ol/li/div/div[@class="c-headline__content"]/a/h2')
  nodes_links <- html_nodes(pagina, xpath = '//ol/li/div/div[@class="c-headline__content"]/a')

  titulos <- html_text(nodes_titulos)
  links <- html_attr(nodes_links, name = "href")
 
  tabela_titulos <- data.frame(titulos, links)
  
  dados_pesquisa <- bind_rows(dados_pesquisa, tabela_titulos)
}

```

Pronto! Temos agora todos os títulos e links de todos os resultados do site da Folha de São Paulo para a palavra "merenda" em um único banco de dados.
